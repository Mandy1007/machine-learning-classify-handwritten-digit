{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import gzip\n",
    "import numpy as np\n",
    "import cPickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class costQuadratic(object):\n",
    "    @staticmethod\n",
    "    #y is the real value, a is the estimate value from neural network\n",
    "    def realCost(a,y):\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "    @staticmethod\n",
    "    def delta(z,a,y):\n",
    "        #z is the putput from the previous layer\n",
    "        return (a-y)*dif_sigmoid(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class costCrossEntropy(object):\n",
    "    @staticmethod\n",
    "    def realCost(a,y):\n",
    "        return np.sum(np.nan_to_sum(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    @staticmethod\n",
    "    def delta(z,a,y):\n",
    "        return (a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f=gzip.open('../mnist.pkl.gz', 'rb')\n",
    "    training_data,validation_data,test_data= cPickle.load(f)\n",
    "    f.close()\n",
    "    return (training_data,validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make the result into a evector\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data,test_data=load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self,sizes,cost=costCrossEntropy):\n",
    "        #sizes is something like [784,20,10], so the first \n",
    "        #layer has 784 nodes, second layer has 20 layers\n",
    "        #and output is 10 layers\n",
    "        self.num_layers=len(sizes)\n",
    "        self.sizes=sizes\n",
    "        self.default_weight_initializer()\n",
    "        #the above function will initialize the weight differently\n",
    "        self.cost=cost\n",
    "    def default_weight_initializer(self):\n",
    "        self.biases=[np.random.randn(y,1) for y in self.sizes[1:]]\n",
    "        \n",
    "        \n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "    def large_weight_initializer(self):\n",
    "        self.biases=[np.random.randn(y,1) for y in self.sizes[1:]]\n",
    "        self.weights=[np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "    def feedforward(self,a):\n",
    "        #a is from the input layer,\n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "            a=sigmoid(np.dot(w,a)+b)\n",
    "        #after this loop,a is the output layer\n",
    "        return a\n",
    "    def SGD(self, training_data,epoches,mini_batch_size,ita,lamda=0.0,\n",
    "           evaluation_data=None,monitor_evaluation_cost=False,\n",
    "           monitor_evaluation_accuracy=False,monitor_training_cost=False,\n",
    "           monitor_training_accuracy=False):\n",
    "        #mini-batch size is the mini size for stochatic gradient descent\n",
    "        if evaluation_data:n_data=len(evaluation_data)\n",
    "        else:n_data=0\n",
    "        n=len(training_data)\n",
    "        training_cost, training_accuracy=[],[]\n",
    "        evaluation_cost,evaluation_accuracy=[],[]\n",
    "        for k in xrange(epoches):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batch_array=[training_data[i:i+mini_batch_size]\n",
    "                             for i in xrange(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batch_array:\n",
    "                self.update_mini_parameters(mini_batch,ita,lamda,n_data)\n",
    "            print \"Epoch %s training finished\" %k\n",
    "            if monitor_training_cost:\n",
    "                cost=self.total_cost(training_data,lamda)\n",
    "                training_cost.append(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy=self.accuracy(training_data,convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost=self.total_cost(evalucation_data,lamda,convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy=self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        return evaluation_cost,evaluation_accuracy,training_cost,training_accuracy\n",
    "    def update_mini_parameters(self,mini_batch,ita,lamda,wholeRecordCount):\n",
    "        pre_update_b=[np.zeros(b.shape) for b in self.biases]\n",
    "        #this is the way to create the same shape of matrix of biases and weight\n",
    "        pre_update_w=[np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x,y in mini_batch:\n",
    "            delta_pre_update_b,delta_pre_update_w=self.backProp(x,y)\n",
    "            pre_update_b=[b+db for b,db in zip(pre_update_b,delta_pre_update_b)]\n",
    "            pre_update_w=[w+dw for w,dw in zip(pre_update_w,delta_pre_update_w)]\n",
    "        self.weights=[(1-ita*(lamda/wholeRecordCount))*w-(ita/len(mini_batch))*deltaW \n",
    "                       for w,deltaW in zip(self.weights,pre_update_w)]\n",
    "        #here divide by len(min_batch) is to average the delta over the minibactch\n",
    "        #then we do update weights and biases later\n",
    "        #we don't do regularization for b, according to the reason in the book\n",
    "        self.biases=[b-(ita/len(mini_batch))*deltaB \n",
    "                    for b, deltaB in zip(self.biases,pre_update_b)]\n",
    "    \n",
    "    def backProp(self,x,y):\n",
    "        #return the gradient of weights and biases for cost function\n",
    "        #this is just for one record, so in order to do derivative\n",
    "        #on the average cost function then get the gradient, is\n",
    "        #equivalent to do the gradient calculation one by one and then take the\n",
    "        #average between the gradient,which will be done in update_mini_parameters\n",
    "        pre_update_b=[np.zeros(b.shape) for b in self.biases]\n",
    "        pre_update_w=[np.zeros(w.shape) for w in self.weights]\n",
    "        #feedforward now\n",
    "        activation=x\n",
    "        activations=[x] \n",
    "        #this is a matrix to store all the act\n",
    "        #ivations, layer by layer\n",
    "        sumActions=[] \n",
    "        #matrix to store all the sum whcih is before we take the sigmoid function\n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "            sumAction=np.dot(w,activation)+b\n",
    "            sumActions.append(sumAction)\n",
    "            activation=sigmoid(sumAction)\n",
    "            activations.append(activation)\n",
    "        #backward\n",
    "        #the following structure set up in order for us to use \n",
    "        #different cost function,say quadratic cost function delta need to \n",
    "        #multiply the sigmoid_prime and crossEntropy don't need to do that\n",
    "        delta_depend_costFunction=(self.cost).delta(sumActions[-1],activations[-1],y)\n",
    "        pre_update_b[-1]=delta_depend_costFunction\n",
    "        pre_update_w[-1]=np.dot(delta_depend_costFunction,activations[-2].transpose())\n",
    "        #testing the feature of np.dot like this, so say I have 784,5,10 sizes\n",
    "        #layerso now delta is 10*1 and activations[-2].transpose is 1*5 so we \n",
    "        #get a 10*5 matrix after the dot and delta for each node in the hidden layer will\n",
    "        #be on the column of this matrix\n",
    "        #the above is about the last layer so it is the initialization of delta\n",
    "        #now we need to calculate delta for other layers\n",
    "        for l in xrange(2,self.num_layers):\n",
    "            sumAction=sumActions[-l]\n",
    "            sp=diff_sigmoid(sumAction)\n",
    "            #here weights.transpose shape is (5,10) and delta is (10,5),sp is (5,1),so we will get a (5,1)\n",
    "            #this is the delta for the hidden layer example\n",
    "            delta_depend_costFunction=np.dot(self.weights[-l+1].transpose(),delta_depend_costFunction)*sp\n",
    "            pre_update_b[-l]=delta_depend_costFunction\n",
    "            #delta is (5,1) and activations[-l-1] is (10,1), so we need a transpose,so delta_weight is (5,10),\n",
    "            #the same as our weight matrix setup.(which the weight set up is the inverse of the formula set up)\n",
    "            pre_update_w[-l]=np.dot(delta_depend_costFunction,activations[-l-1].transpose())\n",
    "            #so the idea is the sigmoid_prime is from the current layer, x(i) is from the previous layer (-l-1) and\n",
    "            #the w(jk) is from the later layer, look at the formula from Dr.Amy\n",
    "        #pre_update_b has the same shape with biases\n",
    "        #pre_update_w has the same shape with weights\n",
    "        return (pre_update_b,pre_update_w)\n",
    "       \n",
    "    def accuracy(self,data,convert=False):\n",
    "        #return the number of record the network predict correct\n",
    "        #convert is False means this accuracy is for test or validation data\n",
    "        #since the usual case is to see the accuracy on test data\n",
    "        #teh set up for traning data is different from the set of testing data\n",
    "        #since we need to do matrix multiplication on the output vector \n",
    "        #in the training data\n",
    "\n",
    "        if convert:\n",
    "            results=[(np.argmax(self.feedforward(x)),np.argmax(y)) for (x,y) in data]\n",
    "        else:\n",
    "            results=[(np.argmax(self.feedforward(x)),y) for (x,y) in data]\n",
    "        \n",
    "        return sum(int(x==y) for (x,y) in results)\n",
    "    def total_cost(self,data,lamda,convert=False):\n",
    "        #the default total cost is on the validation data\n",
    "        #and this is the cost oevr all the data,not the stochastic gradient one\n",
    "        cost=0.0\n",
    "        for x,y in data:\n",
    "            estimated=self.feedforward(x)\n",
    "            if convert: y=vectorized_result(y)\n",
    "            cost=cost+self.cost.realCost/len(data)\n",
    "        #the following is to add in the regularization part\n",
    "        cost=cost+0.5*(lamda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
    "        #so the above sum part is a good way to sum up all the element in a matrix\n",
    "        return cost\n",
    "    \n",
    "    #save the training result to a file\n",
    "    def save(self,filename):\n",
    "        #put the data in the dictionary\n",
    "        data={\n",
    "            \"sizes\":self.sizes,\n",
    "            \"weights\":[w.tolist() for w in self.weights],\n",
    "            \"biases\":[b.tolist() for b in self.biases],\n",
    "            \"cost\":str(self.cost.__name__)\n",
    "            #above is kind of reflection to get the costFunctionName\n",
    "        }\n",
    "        f=open(filename,\"w\")\n",
    "        json.dump(data,f)\n",
    "        f.close()\n",
    "    #the following is load in an already trained network\n",
    "    def load(filename):\n",
    "        #return an instance of the network\n",
    "        f=open(filename,\"r\")\n",
    "        data=json.load(f)\n",
    "        f.close()\n",
    "        cost=getattr(sys.modules[__name__],data[\"cost\"])\n",
    "        #load in the cost function for the network\n",
    "        net=Network(data[\"sizes\"],cost=cost)\n",
    "        net.weights=[np.array(w) for w in data[\"weights\"]]\n",
    "        net.biases=[np.array(b) for b in data[\"biases\"]]\n",
    "        return net\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=np.reshape(data_test[0][0][0],(784,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=data_test[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Network([784, 5, 10], cost=costCrossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 784)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=net.backProp(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.23556702],\n",
       "       [-4.25745572],\n",
       "       [-4.0979444 ],\n",
       "       [-4.26282567],\n",
       "       [-4.75326355],\n",
       "       [-4.46624455],\n",
       "       [-4.69541625],\n",
       "       [-4.17206748],\n",
       "       [-4.86077479],\n",
       "       [-4.60826392]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27473737],\n",
       "       [ 0.57892272],\n",
       "       [ 0.59674144],\n",
       "       [ 0.71654454],\n",
       "       [ 0.29883683]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdot=np.dot(test[0],test[1].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16366853, -2.45206598, -2.52753838, -3.03497241, -1.26574341],\n",
       "       [-1.16968218, -2.46473784, -2.54060027, -3.05065663, -1.27228456],\n",
       "       [-1.12585846, -2.37239312, -2.44541326, -2.93635967, -1.2246167 ],\n",
       "       [-1.1711575 , -2.46784663, -2.54380474, -3.05450445, -1.2738893 ],\n",
       "       [-1.30589912, -2.75177226, -2.83646935, -3.40592503, -1.4204502 ],\n",
       "       [-1.22704427, -2.58561044, -2.66519322, -3.20026313, -1.33467835],\n",
       "       [-1.2900063 , -2.71828314, -2.80194947, -3.36447486, -1.40316329],\n",
       "       [-1.14622284, -2.41530465, -2.48964557, -2.98947216, -1.24676741],\n",
       "       [-1.33543647, -2.81401296, -2.90062576, -3.48296162, -1.45257852],\n",
       "       [-1.2660623 , -2.66782868, -2.74994206, -3.30202634, -1.37711897]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdot=np.dot(test[1],test[0].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16366853, -1.16968218, -1.12585846, -1.1711575 , -1.30589912,\n",
       "        -1.22704427, -1.2900063 , -1.14622284, -1.33543647, -1.2660623 ],\n",
       "       [-2.45206598, -2.46473784, -2.37239312, -2.46784663, -2.75177226,\n",
       "        -2.58561044, -2.71828314, -2.41530465, -2.81401296, -2.66782868],\n",
       "       [-2.52753838, -2.54060027, -2.44541326, -2.54380474, -2.83646935,\n",
       "        -2.66519322, -2.80194947, -2.48964557, -2.90062576, -2.74994206],\n",
       "       [-3.03497241, -3.05065663, -2.93635967, -3.05450445, -3.40592503,\n",
       "        -3.20026313, -3.36447486, -2.98947216, -3.48296162, -3.30202634],\n",
       "       [-1.26574341, -1.27228456, -1.2246167 , -1.2738893 , -1.4204502 ,\n",
       "        -1.33467835, -1.40316329, -1.24676741, -1.45257852, -1.37711897]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training finished\n",
      "Accuracy on evaluation data: 8420 / 10000\n",
      "\n",
      "Epoch 1 training finished\n",
      "Accuracy on evaluation data: 8836 / 10000\n",
      "\n",
      "Epoch 2 training finished\n",
      "Accuracy on evaluation data: 8980 / 10000\n",
      "\n",
      "Epoch 3 training finished\n",
      "Accuracy on evaluation data: 9067 / 10000\n",
      "\n",
      "Epoch 4 training finished\n",
      "Accuracy on evaluation data: 9111 / 10000\n",
      "\n",
      "Epoch 5 training finished\n",
      "Accuracy on evaluation data: 9119 / 10000\n",
      "\n",
      "Epoch 6 training finished\n",
      "Accuracy on evaluation data: 9197 / 10000\n",
      "\n",
      "Epoch 7 training finished\n",
      "Accuracy on evaluation data: 9217 / 10000\n",
      "\n",
      "Epoch 8 training finished\n",
      "Accuracy on evaluation data: 9242 / 10000\n",
      "\n",
      "Epoch 9 training finished\n",
      "Accuracy on evaluation data: 9209 / 10000\n",
      "\n",
      "Epoch 10 training finished\n",
      "Accuracy on evaluation data: 9252 / 10000\n",
      "\n",
      "Epoch 11 training finished\n",
      "Accuracy on evaluation data: 9260 / 10000\n",
      "\n",
      "Epoch 12 training finished\n",
      "Accuracy on evaluation data: 9248 / 10000\n",
      "\n",
      "Epoch 13 training finished\n",
      "Accuracy on evaluation data: 9229 / 10000\n",
      "\n",
      "Epoch 14 training finished\n",
      "Accuracy on evaluation data: 9282 / 10000\n",
      "\n",
      "Epoch 15 training finished\n",
      "Accuracy on evaluation data: 9259 / 10000\n",
      "\n",
      "Epoch 16 training finished\n",
      "Accuracy on evaluation data: 9258 / 10000\n",
      "\n",
      "Epoch 17 training finished\n",
      "Accuracy on evaluation data: 9285 / 10000\n",
      "\n",
      "Epoch 18 training finished\n",
      "Accuracy on evaluation data: 9261 / 10000\n",
      "\n",
      "Epoch 19 training finished\n",
      "Accuracy on evaluation data: 9260 / 10000\n",
      "\n",
      "Epoch 20 training finished\n",
      "Accuracy on evaluation data: 9283 / 10000\n",
      "\n",
      "Epoch 21 training finished\n",
      "Accuracy on evaluation data: 9295 / 10000\n",
      "\n",
      "Epoch 22 training finished\n",
      "Accuracy on evaluation data: 9292 / 10000\n",
      "\n",
      "Epoch 23 training finished\n",
      "Accuracy on evaluation data: 9266 / 10000\n",
      "\n",
      "Epoch 24 training finished\n",
      "Accuracy on evaluation data: 9299 / 10000\n",
      "\n",
      "Epoch 25 training finished\n",
      "Accuracy on evaluation data: 9283 / 10000\n",
      "\n",
      "Epoch 26 training finished\n",
      "Accuracy on evaluation data: 9273 / 10000\n",
      "\n",
      "Epoch 27 training finished\n",
      "Accuracy on evaluation data: 9252 / 10000\n",
      "\n",
      "Epoch 28 training finished\n",
      "Accuracy on evaluation data: 9267 / 10000\n",
      "\n",
      "Epoch 29 training finished\n",
      "Accuracy on evaluation data: 9249 / 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 10, 10], cost=costCrossEntropy)\n",
    "net.large_weight_initializer()\n",
    "oneHidden10=net.SGD(training_data, 30, 10, 0.1, lamda = 5.0,\n",
    "        evaluation_data=validation_data, \n",
    "        monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training finished\n",
      "Accuracy on evaluation data: 9087 / 10000\n",
      "\n",
      "Epoch 1 training finished\n",
      "Accuracy on evaluation data: 9292 / 10000\n",
      "\n",
      "Epoch 2 training finished\n",
      "Accuracy on evaluation data: 9458 / 10000\n",
      "\n",
      "Epoch 3 training finished\n",
      "Accuracy on evaluation data: 9533 / 10000\n",
      "\n",
      "Epoch 4 training finished\n",
      "Accuracy on evaluation data: 9569 / 10000\n",
      "\n",
      "Epoch 5 training finished\n",
      "Accuracy on evaluation data: 9587 / 10000\n",
      "\n",
      "Epoch 6 training finished\n",
      "Accuracy on evaluation data: 9608 / 10000\n",
      "\n",
      "Epoch 7 training finished\n",
      "Accuracy on evaluation data: 9603 / 10000\n",
      "\n",
      "Epoch 8 training finished\n",
      "Accuracy on evaluation data: 9623 / 10000\n",
      "\n",
      "Epoch 9 training finished\n",
      "Accuracy on evaluation data: 9640 / 10000\n",
      "\n",
      "Epoch 10 training finished\n",
      "Accuracy on evaluation data: 9641 / 10000\n",
      "\n",
      "Epoch 11 training finished\n",
      "Accuracy on evaluation data: 9677 / 10000\n",
      "\n",
      "Epoch 12 training finished\n",
      "Accuracy on evaluation data: 9673 / 10000\n",
      "\n",
      "Epoch 13 training finished\n",
      "Accuracy on evaluation data: 9665 / 10000\n",
      "\n",
      "Epoch 14 training finished\n",
      "Accuracy on evaluation data: 9681 / 10000\n",
      "\n",
      "Epoch 15 training finished\n",
      "Accuracy on evaluation data: 9651 / 10000\n",
      "\n",
      "Epoch 16 training finished\n",
      "Accuracy on evaluation data: 9682 / 10000\n",
      "\n",
      "Epoch 17 training finished\n",
      "Accuracy on evaluation data: 9682 / 10000\n",
      "\n",
      "Epoch 18 training finished\n",
      "Accuracy on evaluation data: 9677 / 10000\n",
      "\n",
      "Epoch 19 training finished\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "\n",
      "Epoch 20 training finished\n",
      "Accuracy on evaluation data: 9696 / 10000\n",
      "\n",
      "Epoch 21 training finished\n",
      "Accuracy on evaluation data: 9698 / 10000\n",
      "\n",
      "Epoch 22 training finished\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "\n",
      "Epoch 23 training finished\n",
      "Accuracy on evaluation data: 9685 / 10000\n",
      "\n",
      "Epoch 24 training finished\n",
      "Accuracy on evaluation data: 9697 / 10000\n",
      "\n",
      "Epoch 25 training finished\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "\n",
      "Epoch 26 training finished\n",
      "Accuracy on evaluation data: 9700 / 10000\n",
      "\n",
      "Epoch 27 training finished\n",
      "Accuracy on evaluation data: 9709 / 10000\n",
      "\n",
      "Epoch 28 training finished\n",
      "Accuracy on evaluation data: 9704 / 10000\n",
      "\n",
      "Epoch 29 training finished\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 100, 10], cost=costCrossEntropy)\n",
    "net.large_weight_initializer()\n",
    "oneHidden100=net.SGD(training_data, 30, 10, 0.1, lamda = 5.0,\n",
    "        evaluation_data=validation_data, \n",
    "        monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training finished\n",
      "Accuracy on evaluation data: 9110 / 10000\n",
      "\n",
      "Epoch 1 training finished\n",
      "Accuracy on evaluation data: 9363 / 10000\n",
      "\n",
      "Epoch 2 training finished\n",
      "Accuracy on evaluation data: 9500 / 10000\n",
      "\n",
      "Epoch 3 training finished\n",
      "Accuracy on evaluation data: 9554 / 10000\n",
      "\n",
      "Epoch 4 training finished\n",
      "Accuracy on evaluation data: 9570 / 10000\n",
      "\n",
      "Epoch 5 training finished\n",
      "Accuracy on evaluation data: 9610 / 10000\n",
      "\n",
      "Epoch 6 training finished\n",
      "Accuracy on evaluation data: 9605 / 10000\n",
      "\n",
      "Epoch 7 training finished\n",
      "Accuracy on evaluation data: 9654 / 10000\n",
      "\n",
      "Epoch 8 training finished\n",
      "Accuracy on evaluation data: 9637 / 10000\n",
      "\n",
      "Epoch 9 training finished\n",
      "Accuracy on evaluation data: 9661 / 10000\n",
      "\n",
      "Epoch 10 training finished\n",
      "Accuracy on evaluation data: 9667 / 10000\n",
      "\n",
      "Epoch 11 training finished\n",
      "Accuracy on evaluation data: 9649 / 10000\n",
      "\n",
      "Epoch 12 training finished\n",
      "Accuracy on evaluation data: 9674 / 10000\n",
      "\n",
      "Epoch 13 training finished\n",
      "Accuracy on evaluation data: 9681 / 10000\n",
      "\n",
      "Epoch 14 training finished\n",
      "Accuracy on evaluation data: 9645 / 10000\n",
      "\n",
      "Epoch 15 training finished\n",
      "Accuracy on evaluation data: 9696 / 10000\n",
      "\n",
      "Epoch 16 training finished\n",
      "Accuracy on evaluation data: 9663 / 10000\n",
      "\n",
      "Epoch 17 training finished\n",
      "Accuracy on evaluation data: 9680 / 10000\n",
      "\n",
      "Epoch 18 training finished\n",
      "Accuracy on evaluation data: 9680 / 10000\n",
      "\n",
      "Epoch 19 training finished\n",
      "Accuracy on evaluation data: 9693 / 10000\n",
      "\n",
      "Epoch 20 training finished\n",
      "Accuracy on evaluation data: 9697 / 10000\n",
      "\n",
      "Epoch 21 training finished\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "\n",
      "Epoch 22 training finished\n",
      "Accuracy on evaluation data: 9704 / 10000\n",
      "\n",
      "Epoch 23 training finished\n",
      "Accuracy on evaluation data: 9663 / 10000\n",
      "\n",
      "Epoch 24 training finished\n",
      "Accuracy on evaluation data: 9675 / 10000\n",
      "\n",
      "Epoch 25 training finished\n",
      "Accuracy on evaluation data: 9682 / 10000\n",
      "\n",
      "Epoch 26 training finished\n",
      "Accuracy on evaluation data: 9674 / 10000\n",
      "\n",
      "Epoch 27 training finished\n",
      "Accuracy on evaluation data: 9712 / 10000\n",
      "\n",
      "Epoch 28 training finished\n",
      "Accuracy on evaluation data: 9690 / 10000\n",
      "\n",
      "Epoch 29 training finished\n",
      "Accuracy on evaluation data: 9692 / 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 200, 10], cost=costCrossEntropy)\n",
    "net.large_weight_initializer()\n",
    "oneHidden200=net.SGD(training_data, 30, 10, 0.1, lamda = 5.0,\n",
    "        evaluation_data=validation_data, \n",
    "        monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training finished\n",
      "Accuracy on evaluation data: 8621 / 10000\n",
      "\n",
      "Epoch 1 training finished\n",
      "Accuracy on evaluation data: 9090 / 10000\n",
      "\n",
      "Epoch 2 training finished\n",
      "Accuracy on evaluation data: 9218 / 10000\n",
      "\n",
      "Epoch 3 training finished\n",
      "Accuracy on evaluation data: 9343 / 10000\n",
      "\n",
      "Epoch 4 training finished\n",
      "Accuracy on evaluation data: 9374 / 10000\n",
      "\n",
      "Epoch 5 training finished\n",
      "Accuracy on evaluation data: 9402 / 10000\n",
      "\n",
      "Epoch 6 training finished\n",
      "Accuracy on evaluation data: 9437 / 10000\n",
      "\n",
      "Epoch 7 training finished\n",
      "Accuracy on evaluation data: 9442 / 10000\n",
      "\n",
      "Epoch 8 training finished\n",
      "Accuracy on evaluation data: 9490 / 10000\n",
      "\n",
      "Epoch 9 training finished\n",
      "Accuracy on evaluation data: 9497 / 10000\n",
      "\n",
      "Epoch 10 training finished\n",
      "Accuracy on evaluation data: 9535 / 10000\n",
      "\n",
      "Epoch 11 training finished\n",
      "Accuracy on evaluation data: 9491 / 10000\n",
      "\n",
      "Epoch 12 training finished\n",
      "Accuracy on evaluation data: 9504 / 10000\n",
      "\n",
      "Epoch 13 training finished\n",
      "Accuracy on evaluation data: 9489 / 10000\n",
      "\n",
      "Epoch 14 training finished\n",
      "Accuracy on evaluation data: 9550 / 10000\n",
      "\n",
      "Epoch 15 training finished\n",
      "Accuracy on evaluation data: 9534 / 10000\n",
      "\n",
      "Epoch 16 training finished\n",
      "Accuracy on evaluation data: 9543 / 10000\n",
      "\n",
      "Epoch 17 training finished\n",
      "Accuracy on evaluation data: 9552 / 10000\n",
      "\n",
      "Epoch 18 training finished\n",
      "Accuracy on evaluation data: 9548 / 10000\n",
      "\n",
      "Epoch 19 training finished\n",
      "Accuracy on evaluation data: 9585 / 10000\n",
      "\n",
      "Epoch 20 training finished\n",
      "Accuracy on evaluation data: 9568 / 10000\n",
      "\n",
      "Epoch 21 training finished\n",
      "Accuracy on evaluation data: 9550 / 10000\n",
      "\n",
      "Epoch 22 training finished\n",
      "Accuracy on evaluation data: 9557 / 10000\n",
      "\n",
      "Epoch 23 training finished\n",
      "Accuracy on evaluation data: 9545 / 10000\n",
      "\n",
      "Epoch 24 training finished\n",
      "Accuracy on evaluation data: 9560 / 10000\n",
      "\n",
      "Epoch 25 training finished\n",
      "Accuracy on evaluation data: 9526 / 10000\n",
      "\n",
      "Epoch 26 training finished\n",
      "Accuracy on evaluation data: 9556 / 10000\n",
      "\n",
      "Epoch 27 training finished\n",
      "Accuracy on evaluation data: 9562 / 10000\n",
      "\n",
      "Epoch 28 training finished\n",
      "Accuracy on evaluation data: 9540 / 10000\n",
      "\n",
      "Epoch 29 training finished\n",
      "Accuracy on evaluation data: 9561 / 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 20,20, 10], cost=costCrossEntropy)\n",
    "net.large_weight_initializer()\n",
    "twoHidden2020=net.SGD(training_data, 30, 10, 0.1, lamda = 5.0,\n",
    "        evaluation_data=validation_data, \n",
    "        monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training finished\n",
      "Accuracy on evaluation data: 8792 / 10000\n",
      "\n",
      "Epoch 1 training finished\n",
      "Accuracy on evaluation data: 9121 / 10000\n",
      "\n",
      "Epoch 2 training finished\n",
      "Accuracy on evaluation data: 9363 / 10000\n",
      "\n",
      "Epoch 3 training finished\n",
      "Accuracy on evaluation data: 9426 / 10000\n",
      "\n",
      "Epoch 4 training finished\n",
      "Accuracy on evaluation data: 9493 / 10000\n",
      "\n",
      "Epoch 5 training finished\n",
      "Accuracy on evaluation data: 9554 / 10000\n",
      "\n",
      "Epoch 6 training finished\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "\n",
      "Epoch 7 training finished\n",
      "Accuracy on evaluation data: 9632 / 10000\n",
      "\n",
      "Epoch 8 training finished\n",
      "Accuracy on evaluation data: 9619 / 10000\n",
      "\n",
      "Epoch 9 training finished\n",
      "Accuracy on evaluation data: 9586 / 10000\n",
      "\n",
      "Epoch 10 training finished\n",
      "Accuracy on evaluation data: 9650 / 10000\n",
      "\n",
      "Epoch 11 training finished\n",
      "Accuracy on evaluation data: 9664 / 10000\n",
      "\n",
      "Epoch 12 training finished\n",
      "Accuracy on evaluation data: 9649 / 10000\n",
      "\n",
      "Epoch 13 training finished\n",
      "Accuracy on evaluation data: 9684 / 10000\n",
      "\n",
      "Epoch 14 training finished\n",
      "Accuracy on evaluation data: 9657 / 10000\n",
      "\n",
      "Epoch 15 training finished\n",
      "Accuracy on evaluation data: 9673 / 10000\n",
      "\n",
      "Epoch 16 training finished\n",
      "Accuracy on evaluation data: 9687 / 10000\n",
      "\n",
      "Epoch 17 training finished\n",
      "Accuracy on evaluation data: 9665 / 10000\n",
      "\n",
      "Epoch 18 training finished\n",
      "Accuracy on evaluation data: 9693 / 10000\n",
      "\n",
      "Epoch 19 training finished\n",
      "Accuracy on evaluation data: 9692 / 10000\n",
      "\n",
      "Epoch 20 training finished\n",
      "Accuracy on evaluation data: 9691 / 10000\n",
      "\n",
      "Epoch 21 training finished\n",
      "Accuracy on evaluation data: 9688 / 10000\n",
      "\n",
      "Epoch 22 training finished\n",
      "Accuracy on evaluation data: 9691 / 10000\n",
      "\n",
      "Epoch 23 training finished\n",
      "Accuracy on evaluation data: 9695 / 10000\n",
      "\n",
      "Epoch 24 training finished\n",
      "Accuracy on evaluation data: 9714 / 10000\n",
      "\n",
      "Epoch 25 training finished\n",
      "Accuracy on evaluation data: 9686 / 10000\n",
      "\n",
      "Epoch 26 training finished\n",
      "Accuracy on evaluation data: 9705 / 10000\n",
      "\n",
      "Epoch 27 training finished\n",
      "Accuracy on evaluation data: 9704 / 10000\n",
      "\n",
      "Epoch 28 training finished\n",
      "Accuracy on evaluation data: 9695 / 10000\n",
      "\n",
      "Epoch 29 training finished\n",
      "Accuracy on evaluation data: 9663 / 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 50,20, 10], cost=costCrossEntropy)\n",
    "net.large_weight_initializer()\n",
    "twoHidden5020=net.SGD(training_data, 30, 10, 0.1, lamda = 5.0,\n",
    "        evaluation_data=validation_data, \n",
    "        monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training finished\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 1 training finished\n",
      "Accuracy on evaluation data: 8887 / 10000\n",
      "\n",
      "Epoch 2 training finished\n",
      "Accuracy on evaluation data: 9089 / 10000\n",
      "\n",
      "Epoch 3 training finished\n",
      "Accuracy on evaluation data: 9171 / 10000\n",
      "\n",
      "Epoch 4 training finished\n",
      "Accuracy on evaluation data: 9229 / 10000\n",
      "\n",
      "Epoch 5 training finished\n",
      "Accuracy on evaluation data: 9263 / 10000\n",
      "\n",
      "Epoch 6 training finished\n",
      "Accuracy on evaluation data: 9310 / 10000\n",
      "\n",
      "Epoch 7 training finished\n",
      "Accuracy on evaluation data: 9332 / 10000\n",
      "\n",
      "Epoch 8 training finished\n",
      "Accuracy on evaluation data: 9357 / 10000\n",
      "\n",
      "Epoch 9 training finished\n",
      "Accuracy on evaluation data: 9394 / 10000\n",
      "\n",
      "Epoch 10 training finished\n",
      "Accuracy on evaluation data: 9392 / 10000\n",
      "\n",
      "Epoch 11 training finished\n",
      "Accuracy on evaluation data: 9383 / 10000\n",
      "\n",
      "Epoch 12 training finished\n",
      "Accuracy on evaluation data: 9416 / 10000\n",
      "\n",
      "Epoch 13 training finished\n",
      "Accuracy on evaluation data: 9434 / 10000\n",
      "\n",
      "Epoch 14 training finished\n",
      "Accuracy on evaluation data: 9400 / 10000\n",
      "\n",
      "Epoch 15 training finished\n",
      "Accuracy on evaluation data: 9434 / 10000\n",
      "\n",
      "Epoch 16 training finished\n",
      "Accuracy on evaluation data: 9413 / 10000\n",
      "\n",
      "Epoch 17 training finished\n",
      "Accuracy on evaluation data: 9447 / 10000\n",
      "\n",
      "Epoch 18 training finished\n",
      "Accuracy on evaluation data: 9448 / 10000\n",
      "\n",
      "Epoch 19 training finished\n",
      "Accuracy on evaluation data: 9454 / 10000\n",
      "\n",
      "Epoch 20 training finished\n",
      "Accuracy on evaluation data: 9449 / 10000\n",
      "\n",
      "Epoch 21 training finished\n",
      "Accuracy on evaluation data: 9465 / 10000\n",
      "\n",
      "Epoch 22 training finished\n",
      "Accuracy on evaluation data: 9460 / 10000\n",
      "\n",
      "Epoch 23 training finished\n",
      "Accuracy on evaluation data: 9447 / 10000\n",
      "\n",
      "Epoch 24 training finished\n",
      "Accuracy on evaluation data: 9478 / 10000\n",
      "\n",
      "Epoch 25 training finished\n",
      "Accuracy on evaluation data: 9475 / 10000\n",
      "\n",
      "Epoch 26 training finished\n",
      "Accuracy on evaluation data: 9477 / 10000\n",
      "\n",
      "Epoch 27 training finished\n",
      "Accuracy on evaluation data: 9467 / 10000\n",
      "\n",
      "Epoch 28 training finished\n",
      "Accuracy on evaluation data: 9479 / 10000\n",
      "\n",
      "Epoch 29 training finished\n",
      "Accuracy on evaluation data: 9489 / 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#no regularization case:\n",
    "net = Network([784, 50,20, 10], cost=costCrossEntropy)\n",
    "net.large_weight_initializer()\n",
    "twoHidden5020_noReg=net.SGD(training_data, 30, 10, 0.1, lamda = 0.0,\n",
    "        evaluation_data=validation_data, \n",
    "        monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def svm_baseline():\n",
    "    training_data, validation_data, test_data = load_data()\n",
    "# train\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(training_data[0], training_data[1])\n",
    "# test\n",
    "    predictions = [int(a) for a in clf.predict(test_data[0])]\n",
    "    num_correct = sum(int(a == y) for a, y in zip(predictions, test_data[1]))\n",
    "    print \"Baseline classifier using an SVM.\"\n",
    "    print \"%s of %s values correct.\" % (num_correct, len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classifier using an SVM.\n",
      "9435 of 10000 values correct.\n"
     ]
    }
   ],
   "source": [
    "svm_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def giveMeThePercentage(arrayToChange):\n",
    "    return [round(x/10000.0,3) for x in arrayToChange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oneHidden10Per=giveMeThePercentage(oneHidden10[1])\n",
    "oneHidden100Per=giveMeThePercentage(oneHidden100[1])\n",
    "oneHidden200Per=giveMeThePercentage(oneHidden200[1])\n",
    "twoHidden2020Per=giveMeThePercentage(twoHidden2020[1])\n",
    "twoHidden5020Per=giveMeThePercentage(twoHidden5020[1])\n",
    "twoHidden5020Per_noReg=giveMeThePercentage(twoHidden5020_noReg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(oneHidden10Per,'r',label=\"1 hidden 10\")\n",
    "plt.plot(oneHidden100Per,'g',label=\"1 hidden 100\")\n",
    "plt.plot(oneHidden200Per,'b',label=\"1 hidden 200\")\n",
    "plt.plot(twoHidden2020Per,'y',label=\"2 hidden 20 20\")\n",
    "plt.plot(twoHidden5020Per,'k',marker=\"o\",label=\"2 hidden 50 20\")\n",
    "plt.plot(twoHidden5020Per_noReg,'k',marker=\"*\",label=\"2 hidden 50 20 no reg\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def svm_baseline():\n",
    "    training_data, validation_data, test_data = load_data()\n",
    "# train\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(training_data[0], training_data[1])\n",
    "# test\n",
    "    predictions = [int(a) for a in clf.predict(test_data[0])]\n",
    "    num_correct = sum(int(a == y) for a, y in zip(predictions, test_data[1]))\n",
    "    print \"Baseline classifier using an SVM.\"\n",
    "    print \"%s of %s values correct.\" % (num_correct, len(test_data[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
